{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud Detection - Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook contains exploratory analysis only. It is NOT required for training or inference.\n",
        "\n",
        "**Purpose:** Understand data distributions, correlations, class balance, and feature relationships.\n",
        "\n",
        "**Author:** Cristhian Acosta  \n",
        "**Date:** February 2026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "np.random.seed(42)\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "from src.data import prepare_dataset\n",
        "\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process data using production pipeline\n",
        "DATA_PATH = \"../fraud-detection-mlops/data/raw/onlinefraud.csv\"\n",
        "\n",
        "X, y = prepare_dataset(DATA_PATH)\n",
        "\n",
        "print(f\"\\nDataset Shape:\")\n",
        "print(f\"  Features: {X.shape}\")\n",
        "print(f\"  Target: {y.shape}\")\n",
        "print(f\"\\nClass Distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"  Fraud rate: {y.mean():.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Class Balance Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "counts = y.value_counts()\n",
        "axes[0].bar(['Legitimate', 'Fraud'], counts.values, color=['green', 'red'], alpha=0.7)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_title('Class Distribution (Absolute)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Percentage plot\n",
        "axes[1].pie(counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.4f%%',\n",
        "           colors=['green', 'red'], startangle=90)\n",
        "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nDataset is highly imbalanced: {y.mean():.4%} fraud\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Feature Statistics:\\n\")\n",
        "print(X.describe().T)\n",
        "\n",
        "print(f\"\\nMissing Values:\")\n",
        "missing = X.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(missing[missing > 0])\n",
        "else:\n",
        "    print(\"  No missing values found\")\n",
        "\n",
        "print(f\"\\nFeature Types:\")\n",
        "print(X.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select key numerical features for visualization\n",
        "key_features = ['amount', 'log1p_amount', 'balance_change_orig', \n",
        "                'balance_change_dest', 'hour_of_day', 'day_of_month']\n",
        "\n",
        "existing_features = [f for f in key_features if f in X.columns]\n",
        "\n",
        "if len(existing_features) >= 6:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, feature in enumerate(existing_features[:6]):\n",
        "        axes[i].hist(X[feature], bins=50, alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_xlabel(feature, fontsize=11)\n",
        "        axes[i].set_ylabel('Frequency', fontsize=11)\n",
        "        axes[i].set_title(f'{feature} Distribution', fontsize=12, fontweight='bold')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Distributions by Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data for visualization (to avoid memory issues)\n",
        "sample_size = min(10000, len(X))\n",
        "sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
        "X_sample = X.iloc[sample_idx]\n",
        "y_sample = y.iloc[sample_idx]\n",
        "\n",
        "# Combine for easier plotting\n",
        "df_sample = X_sample.copy()\n",
        "df_sample['isFraud'] = y_sample.values\n",
        "\n",
        "# Select features for comparison\n",
        "compare_features = ['log1p_amount', 'balance_change_orig', 'emptied_account', 'is_night']\n",
        "existing_compare = [f for f in compare_features if f in df_sample.columns]\n",
        "\n",
        "if len(existing_compare) >= 4:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, feature in enumerate(existing_compare[:4]):\n",
        "        df_sample[df_sample['isFraud'] == 0][feature].hist(\n",
        "            ax=axes[i], bins=30, alpha=0.5, label='Legitimate', color='green'\n",
        "        )\n",
        "        df_sample[df_sample['isFraud'] == 1][feature].hist(\n",
        "            ax=axes[i], bins=30, alpha=0.5, label='Fraud', color='red'\n",
        "        )\n",
        "        axes[i].set_xlabel(feature, fontsize=11)\n",
        "        axes[i].set_ylabel('Frequency', fontsize=11)\n",
        "        axes[i].set_title(f'{feature} by Class', fontsize=12, fontweight='bold')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute correlation on numeric features only\n",
        "numeric_features = X_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Limit to reasonable number for visualization\n",
        "if len(numeric_features) > 20:\n",
        "    # Select most important features based on correlation with key variables\n",
        "    key_vars = ['amount', 'balance_change_orig', 'balance_change_dest', \n",
        "                'inconsistent_orig', 'inconsistent_dest']\n",
        "    existing_key = [v for v in key_vars if v in numeric_features]\n",
        "    \n",
        "    if existing_key:\n",
        "        corr_full = X_sample[numeric_features].corr()\n",
        "        # Get top correlated with key variables\n",
        "        top_features = set(existing_key)\n",
        "        for key_var in existing_key:\n",
        "            if key_var in corr_full.columns:\n",
        "                top_corr = corr_full[key_var].abs().nlargest(8).index.tolist()\n",
        "                top_features.update(top_corr)\n",
        "        numeric_features = list(top_features)[:20]\n",
        "\n",
        "corr_matrix = X_sample[numeric_features].corr()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
        "           square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated pairs\n",
        "print(\"\\nHighly Correlated Feature Pairs (|r| > 0.9):\\n\")\n",
        "high_corr = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
        "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
        "\n",
        "if high_corr:\n",
        "    for feat1, feat2, corr_val in high_corr:\n",
        "        print(f\"  {feat1:30s} <-> {feat2:30s}  r={corr_val:6.3f}\")\n",
        "else:\n",
        "    print(\"  No pairs with |r| > 0.9 found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Transaction Type Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'type' in X.columns:\n",
        "    df_type = X[['type']].copy()\n",
        "    df_type['isFraud'] = y.values\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Type distribution\n",
        "    type_counts = df_type['type'].value_counts()\n",
        "    axes[0].bar(type_counts.index, type_counts.values, alpha=0.7)\n",
        "    axes[0].set_xlabel('Transaction Type', fontsize=12)\n",
        "    axes[0].set_ylabel('Count', fontsize=12)\n",
        "    axes[0].set_title('Transaction Type Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Fraud rate by type\n",
        "    fraud_rate_by_type = df_type.groupby('type')['isFraud'].mean()\n",
        "    axes[1].bar(fraud_rate_by_type.index, fraud_rate_by_type.values * 100, \n",
        "               color='red', alpha=0.7)\n",
        "    axes[1].set_xlabel('Transaction Type', fontsize=12)\n",
        "    axes[1].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
        "    axes[1].set_title('Fraud Rate by Transaction Type', fontsize=14, fontweight='bold')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nFraud Rate by Type:\")\n",
        "    print(fraud_rate_by_type.to_string())\n",
        "else:\n",
        "    print(\"⚠️  'type' column not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Temporal Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'hour_of_day' in X.columns and 'day_of_month' in X.columns:\n",
        "    df_temporal = X[['hour_of_day', 'day_of_month']].copy()\n",
        "    df_temporal['isFraud'] = y.values\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    \n",
        "    # Fraud by hour\n",
        "    fraud_by_hour = df_temporal.groupby('hour_of_day')['isFraud'].mean() * 100\n",
        "    axes[0].plot(fraud_by_hour.index, fraud_by_hour.values, marker='o', linewidth=2)\n",
        "    axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
        "    axes[0].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
        "    axes[0].set_title('Fraud Rate by Hour of Day', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Fraud by day\n",
        "    fraud_by_day = df_temporal.groupby('day_of_month')['isFraud'].mean() * 100\n",
        "    axes[1].plot(fraud_by_day.index, fraud_by_day.values, marker='o', linewidth=2, color='orange')\n",
        "    axes[1].set_xlabel('Day of Month', fontsize=12)\n",
        "    axes[1].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
        "    axes[1].set_title('Fraud Rate by Day of Month', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Key Feature Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze key fraud indicators\n",
        "indicators = ['emptied_account', 'inconsistent_orig', 'inconsistent_dest', \n",
        "              'dest_account_new', 'amount_is_round']\n",
        "existing_indicators = [ind for ind in indicators if ind in X.columns]\n",
        "\n",
        "if existing_indicators:\n",
        "    df_indicators = X[existing_indicators].copy()\n",
        "    df_indicators['isFraud'] = y.values\n",
        "    \n",
        "    print(\"Fraud Rate by Key Indicators:\\n\")\n",
        "    for indicator in existing_indicators:\n",
        "        fraud_rate = df_indicators.groupby(indicator)['isFraud'].mean()\n",
        "        print(f\"\\n{indicator}:\")\n",
        "        for val, rate in fraud_rate.items():\n",
        "            print(f\"  {val}: {rate:.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Correlation with Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute correlation with target\n",
        "df_with_target = X_sample.copy()\n",
        "df_with_target['isFraud'] = y_sample.values\n",
        "\n",
        "numeric_cols = df_with_target.select_dtypes(include=[np.number]).columns\n",
        "target_corr = df_with_target[numeric_cols].corrwith(df_with_target['isFraud']).sort_values(\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "print(\"\\nTop 15 Features Correlated with Fraud:\\n\")\n",
        "print(target_corr.head(15).to_string())\n",
        "\n",
        "print(\"\\nBottom 10 Features (Negative Correlation):\\n\")\n",
        "print(target_corr.tail(10).to_string())\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "target_corr.drop('isFraud').sort_values().tail(20).plot(kind='barh', color='steelblue')\n",
        "plt.xlabel('Correlation with Fraud', fontsize=12)\n",
        "plt.title('Top 20 Features Correlated with Fraud', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Recommendations\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Class Imbalance:** Dataset is highly imbalanced (~0.13% fraud) → Use SMOTE or class weights\n",
        "2. **Feature Engineering:** Multiple derived features capture balance inconsistencies\n",
        "3. **Temporal Patterns:** Fraud rates may vary by hour/day\n",
        "4. **Transaction Types:** Certain types have higher fraud rates\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Train model with SMOTE or class weights\n",
        "- Monitor AUC-PR (more relevant than AUC-ROC for imbalanced data)\n",
        "- Consider removing highly correlated features if needed\n",
        "- Use optimal threshold from threshold analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
